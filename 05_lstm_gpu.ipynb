{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "from time import perf_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuda is available so we can train on GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define two models. One with dropout and one without. We can use both the same during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers):\n",
    "        super(ModelLSTM, self).__init__()\n",
    "        output_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, n_layers, batch_first=True)   \n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x, full_hidden):\n",
    "        out, full_hidden = self.lstm(x, full_hidden)\n",
    "        out = out.contiguous().view(-1, self.hidden_size)\n",
    "        out = self.fc(out)\n",
    "        return out, full_hidden\n",
    "    \n",
    "    def init_full_hidden(self, batch_size):\n",
    "        hidden = torch.randn(self.n_layers, batch_size, self.hidden_size).to(device)\n",
    "        cell_state = torch.randn(self.n_layers, batch_size, self.hidden_size).to(device)\n",
    "        return (hidden, cell_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelLSTMDrop(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers):\n",
    "        super(ModelLSTMDrop, self).__init__()\n",
    "        output_size = input_size\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, n_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x, full_hidden):\n",
    "        out, full_hidden = self.lstm(x, full_hidden)\n",
    "        out = self.dropout(out)\n",
    "        out = out.contiguous().view(-1, self.hidden_size)\n",
    "        out = self.fc(out)\n",
    "        return out, full_hidden\n",
    "    \n",
    "    def init_full_hidden(self, batch_size):\n",
    "        hidden = torch.randn(self.n_layers, batch_size, self.hidden_size).to(device)\n",
    "        cell_state = torch.randn(self.n_layers, batch_size, self.hidden_size).to(device)\n",
    "        return (hidden, cell_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some helper functions.\n",
    "\n",
    "split_eq - will split the dataset into equall parts and return them\n",
    "\n",
    "translate_to_int - will do char to int translation for multiple multicharacter examples (a -> 3)\n",
    "\n",
    "one_hot_encode does: 5 -> 000010 translation, given that our vocab_size is 6 for example\n",
    "\n",
    "to_model_format - translates a string of text into model understandable format (pytorch Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_eq(text, no):\n",
    "    cnt = int(len(text) / no)\n",
    "    examples = [text[i:i+cnt] for i in range(0, len(text), cnt)]\n",
    "    if (no*cnt == len(text)):\n",
    "        return examples\n",
    "    else:\n",
    "        return examples[:-1]\n",
    "\n",
    "    \n",
    "def produce_targets(examples):\n",
    "    targets = [ex[1:] for ex in examples]\n",
    "    inputs = [ex[:-1] for ex in examples]\n",
    "    return inputs, targets\n",
    "\n",
    "\n",
    "def translate_to_int(examples):\n",
    "    translated = [list(map(lambda ch: char2int[ch], ex)) for ex in examples]\n",
    "    return translated\n",
    "\n",
    "\n",
    "def translate_to_char(examples):\n",
    "    translated = [''.join(list(map(lambda i: int2char[i], ex))) for ex in examples]\n",
    "    return translated\n",
    "\n",
    "\n",
    "def one_hot_encode(arr, vocab_size):\n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((arr.size, vocab_size), dtype=np.float32)\n",
    "\n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "\n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, vocab_size))\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def to_model_format(inputs, vocab_size):\n",
    "    if isinstance(inputs, str):\n",
    "        inputs = [inputs]\n",
    "    trans_inputs = np.array(translate_to_int(inputs))\n",
    "    encoded = one_hot_encode(trans_inputs, vocab_size)\n",
    "    encoded_tensor = torch.from_numpy(encoded)\n",
    "    return encoded_tensor\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration of dataset.\n",
    "\n",
    "No of training chars.\n",
    "No of examples and batch_size.\n",
    "\n",
    "\n",
    "Those are important. Notice that no_of_batches decides how often Neural network is going to backpropagate gradients.\n",
    "You can notice that when looking at the training code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "no_of_chars = 164000\n",
    "no_of_examples = 1024\n",
    "batch_size = examples_per_batch = 128\n",
    "no_of_batches = int(no_of_examples / examples_per_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading dataset, creating dictionaries, spliting dataset into examples and batches.\n",
    "\n",
    "Translating examples into ready-to-use format. (In training only tensors and cuda() is needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./datasets/anna_karenina', 'r', encoding='utf-8') as fd:\n",
    "    full_text = fd.read()\n",
    "    full_text = full_text[0:no_of_chars]\n",
    "\n",
    "vocab = set(full_text)\n",
    "int2char = dict(enumerate(vocab))\n",
    "char2int = {char: ind for ind, char in int2char.items()}\n",
    "vocab_size = len(char2int)\n",
    "print(\"Vocabulary size:\", vocab_size)\n",
    "print(\"Text lenght:\", len(full_text))\n",
    "\n",
    "# TODO this should be fixed: translate once than split\n",
    "examples = split_eq(full_text, no_of_examples)\n",
    "chars_per_example = len(examples[0])\n",
    "inputs, targets = produce_targets(examples)\n",
    "trans_inputs = translate_to_int(inputs)\n",
    "trans_targets = translate_to_int(targets)\n",
    "\n",
    "batches = []\n",
    "\n",
    "for i in range(no_of_batches):\n",
    "    input_seq = one_hot_encode(np.array(trans_inputs[i*examples_per_batch:(i+1)*examples_per_batch]), vocab_size)\n",
    "    target_seq = np.array(trans_targets[i*examples_per_batch:(i+1)*examples_per_batch])\n",
    "    batches.append((input_seq, target_seq))\n",
    "\n",
    "print(\"No of examples/No of data parts:\", no_of_examples)\n",
    "print(\"No of batches:\", no_of_batches)\n",
    "print(\"Examples per batch:\", examples_per_batch)\n",
    "print(\"Chars per example:\", chars_per_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define a network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "model = ModelLSTMDrop(input_size=vocab_size, hidden_size=256, n_layers=3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll train a network. TODO: describe step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 380\n",
    "counter = 0\n",
    "print_every = 4\n",
    "\n",
    "t_start = perf_counter()\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    counter += 1\n",
    "    h = model.init_full_hidden(batch_size)\n",
    "    for batch in batches:\n",
    "        h = tuple([each.data for each in h])\n",
    "        model.zero_grad()\n",
    "        x, y = batch\n",
    "        inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        output, h = model(inputs, h)\n",
    "        loss = criterion(output, targets.view(-1).long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if counter%print_every == 0:\n",
    "        print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
    "              \"Loss: {:.6f}...\".format(loss.item()))\n",
    "        t_stop = perf_counter()\n",
    "        print(\"Time elasped:\", t_stop - t_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: describe functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next(device, model, full_hidden, input_string):\n",
    "    encoded_input = to_model_format(input_string, model.input_size)\n",
    "    out, hidden = model(encoded_input.to(device), full_hidden)\n",
    "\n",
    "    # choosing one with highest probability\n",
    "    prob = nn.functional.softmax(out[-1], dim=0).data\n",
    "    char_ind = torch.max(prob, dim=0)[1].item()\n",
    "    return int2char[char_ind], hidden\n",
    "\n",
    "\n",
    "def run_model(device, model, starting_seq, size=100):\n",
    "    model.eval()\n",
    "    seq = starting_seq.lower()\n",
    "    h = model.init_full_hidden(1)\n",
    "    for _ in range(size):\n",
    "        char, h = predict_next(device, model, h, seq)\n",
    "        seq += char\n",
    "    return ''.join(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = run_model(device, model, 'A great and advanced society has ', 250)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment to save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), \"./models/lstm_gpu_256_2_1964560\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some results:\n",
    "\n",
    "\n",
    "---------- Epoch:80, Chars:164000, No of batches:8, Per batch:128, Layers:2, HiddenSize:256, Loss:1.07\n",
    "\n",
    "a great and advanced society has care theres, and she had been and at the same time with her would be a seated, and he was all the shill was should down to see himself and looking of the conversation. “I thank of at yound there was a conservative, and I have no sint a minute. That I\n",
    "\n",
    "---------- Epoch:80, Chars:164000, No of batches:8, Per batch:128, Layers:3||||, HiddenSize:256, Loss:1.15\n",
    "a great and advanced society has of the same time them one of the said to her. He was as that she was something was a strong and strice all the same time the strain for the same to the same to the same to the same to the same to the same to the same to the same to the same to the sa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions:\n",
    "\n",
    "Increase in number of layers does not always help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
