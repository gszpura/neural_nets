{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "from time import perf_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuda is available so we can train on GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define two models. One with dropout and one without. We can use both the same during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers):\n",
    "        super(ModelLSTM, self).__init__()\n",
    "        output_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, n_layers, batch_first=True)   \n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x, full_hidden):\n",
    "        out, full_hidden = self.lstm(x, full_hidden)\n",
    "        out = out.contiguous().view(-1, self.hidden_size)\n",
    "        out = self.fc(out)\n",
    "        return out, full_hidden\n",
    "    \n",
    "    def init_full_hidden(self, batch_size):\n",
    "        hidden = torch.randn(self.n_layers, batch_size, self.hidden_size).to(device)\n",
    "        cell_state = torch.randn(self.n_layers, batch_size, self.hidden_size).to(device)\n",
    "        return (hidden, cell_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelLSTMDrop(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers):\n",
    "        super(ModelLSTMDrop, self).__init__()\n",
    "        output_size = input_size\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, n_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x, full_hidden):\n",
    "        out, full_hidden = self.lstm(x, full_hidden)\n",
    "        out = self.dropout(out)\n",
    "        out = out.contiguous().view(-1, self.hidden_size)\n",
    "        out = self.fc(out)\n",
    "        return out, full_hidden\n",
    "    \n",
    "    def init_full_hidden(self, batch_size):\n",
    "        hidden = torch.randn(self.n_layers, batch_size, self.hidden_size).to(device)\n",
    "        cell_state = torch.randn(self.n_layers, batch_size, self.hidden_size).to(device)\n",
    "        return (hidden, cell_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some helper functions.\n",
    "\n",
    "split_eq - will split the dataset into equall parts and return them\n",
    "\n",
    "translate_to_int - will do char to int translation for multiple multicharacter examples (a -> 3)\n",
    "\n",
    "one_hot_encode does: 5 -> 000010 translation, given that our vocab_size is 6 for example\n",
    "\n",
    "to_model_format - translates a string of text into model understandable format (pytorch Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_eq(text, no):\n",
    "    cnt = int(len(text) / no)\n",
    "    examples = [text[i:i+cnt] for i in range(0, len(text), cnt)]\n",
    "    if (no*cnt == len(text)):\n",
    "        return examples\n",
    "    else:\n",
    "        return examples[:-1]\n",
    "\n",
    "    \n",
    "def produce_targets(examples):\n",
    "    targets = [ex[1:] for ex in examples]\n",
    "    inputs = [ex[:-1] for ex in examples]\n",
    "    return inputs, targets\n",
    "\n",
    "\n",
    "def translate_to_int(examples):\n",
    "    translated = [list(map(lambda ch: char2int[ch], ex)) for ex in examples]\n",
    "    return translated\n",
    "\n",
    "\n",
    "def translate_to_char(examples):\n",
    "    translated = [''.join(list(map(lambda i: int2char[i], ex))) for ex in examples]\n",
    "    return translated\n",
    "\n",
    "\n",
    "def one_hot_encode(arr, vocab_size):\n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((arr.size, vocab_size), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "\n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, vocab_size))\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def one_hot_encode_old(examples, v):\n",
    "    features = np.zeros((len(examples), len(examples[0]), len(char2int)), dtype=np.float32)\n",
    "    for i, example in enumerate(examples):\n",
    "        for pos in range(len(examples[i]) - 1):\n",
    "            features[i, pos, examples[i][pos]] = 1\n",
    "    return features\n",
    "\n",
    "\n",
    "def to_model_format(inputs, vocab_size):\n",
    "    if isinstance(inputs, str):\n",
    "        inputs = [inputs]\n",
    "    trans_inputs = np.array(translate_to_int(inputs))\n",
    "    encoded = one_hot_encode(trans_inputs, vocab_size)\n",
    "    encoded_tensor = torch.from_numpy(encoded)\n",
    "    return encoded_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration of dataset.\n",
    "\n",
    "No of training chars.\n",
    "No of examples and batch_size.\n",
    "\n",
    "\n",
    "Those are important. Notice that no_of_batches decides how often Neural network is going to backpropagate gradients.\n",
    "You can notice that when looking at the training code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "no_of_chars = 512000\n",
    "no_of_examples = 4096\n",
    "batch_size = examples_per_batch = 128\n",
    "no_of_batches = int(no_of_examples / examples_per_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading dataset, creating dictionaries, spliting dataset into examples and batches.\n",
    "\n",
    "Translating examples into ready-to-use format. (In training only tensors and cuda() is needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./datasets/potop', 'r', encoding='utf-8') as fd:\n",
    "    full_text = fd.read()\n",
    "    full_text = full_text[0:no_of_chars]\n",
    "\n",
    "vocab = set(full_text)\n",
    "int2char = dict(enumerate(vocab))\n",
    "char2int = {char: ind for ind, char in int2char.items()}\n",
    "vocab_size = len(char2int)\n",
    "print(\"Vocabulary size:\", vocab_size)\n",
    "print(\"Text lenght:\", len(full_text))\n",
    "\n",
    "# TODO this should be fixed: translate once than split\n",
    "examples = split_eq(full_text, no_of_examples)\n",
    "chars_per_example = len(examples[0])\n",
    "inputs, targets = produce_targets(examples)\n",
    "trans_inputs = translate_to_int(inputs)\n",
    "trans_targets = translate_to_int(targets)\n",
    "\n",
    "batches = []\n",
    "\n",
    "for i in range(no_of_batches):\n",
    "    input_seq = one_hot_encode(np.array(trans_inputs[i*examples_per_batch:(i+1)*examples_per_batch]), vocab_size)\n",
    "    target_seq = np.array(trans_targets[i*examples_per_batch:(i+1)*examples_per_batch])\n",
    "    batches.append((input_seq, target_seq))\n",
    "\n",
    "print(\"No of examples/No of data parts:\", no_of_examples)\n",
    "print(\"No of batches:\", no_of_batches)\n",
    "print(\"Examples per batch:\", examples_per_batch)\n",
    "print(\"Chars per example:\", chars_per_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define a network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0048\n",
    "model = ModelLSTMDrop(input_size=vocab_size, hidden_size=256, n_layers=3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll train a network. TODO: describe step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 300\n",
    "counter = 0\n",
    "print_every = 10\n",
    "\n",
    "t_start = perf_counter()\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    counter += 1\n",
    "    h = model.init_full_hidden(batch_size)\n",
    "    for batch in batches:\n",
    "        h = tuple([each.data for each in h])\n",
    "        model.zero_grad()\n",
    "        x, y = batch\n",
    "        inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        output, h = model(inputs, h)\n",
    "        loss = criterion(output, targets.view(-1).long())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        optimizer.step()\n",
    "        \n",
    "    if counter%print_every == 0:\n",
    "        print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
    "              \"Loss: {:.6f}...\".format(loss.item()))\n",
    "        t_stop = perf_counter()\n",
    "        print(\"Time elasped:\", t_stop - t_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: describe functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next(device, model, full_hidden, input_string):\n",
    "    encoded_input = to_model_format(input_string, model.input_size)\n",
    "    out, hidden = model(encoded_input.to(device), full_hidden)\n",
    "\n",
    "    # choosing one with highest probability\n",
    "    prob = nn.functional.softmax(out[-1], dim=0).data\n",
    "    char_ind = torch.max(prob, dim=0)[1].item()\n",
    "    return int2char[char_ind], hidden\n",
    "\n",
    "\n",
    "def run_model(device, model, starting_seq, size=100):\n",
    "    model.eval()\n",
    "    seq = starting_seq.lower()\n",
    "    h = model.init_full_hidden(1)\n",
    "    for _ in range(size):\n",
    "        char, h = predict_next(device, model, h, seq)\n",
    "        seq += char\n",
    "    return ''.join(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = run_model(device, model, u'A teraz ', 350)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment to save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), \"./models/lstm_gpu_256_2_1964560\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some results:\n",
    "\n",
    "\n",
    "---------- Epoch:380, Chars:164000, No of batches:8, Per batch:128, Layers:2, HiddenSize:256, Loss:1.07\n",
    "\n",
    "a great and advanced society has care theres, and she had been and at the same time with her would be a seated, and he was all the shill was should down to see himself and looking of the conversation. “I thank of at yound there was a conservative, and I have no sint a minute. That I\n",
    "\n",
    "---------- Epoch:380, Chars:164000, No of batches:8, Per batch:128, Layers:3||||, HiddenSize:256, Loss:1.15\n",
    "\n",
    "a great and advanced society has of the same time them one of the said to her. He was as that she was something was a strong and strice all the same time the strain for the same to the same to the same to the same to the same to the same to the same to the same to the same to the sa\n",
    "\n",
    "---------- Epoch:380, Chars:164000, No of batches:8, Per batch:128, Layers:3, HiddenSize:256, Clip:5||||, Loss:0.92\n",
    "\n",
    "a great and advanced society has a childrend, and she went on offecely, and the cheese that she was anywayd olde, and see with a fo her. And he said, and have been still the princess was all the straning with her fathers, and with a smile, andshe said at the serelinc for the say at that the promestly still burg olesting to him, and somewing fanter the strangess of his onees, and a\n",
    "\n",
    "---------- Epoch:380, Chars:512000||||, No of batches:32, Per batch:128, Layers:3, HiddenSize:256, Clip:5, Loss:0.85\n",
    "\n",
    "a great and advanced society has the opere state, and the sense of her son, and his face was standing at the open possible and strangly with a smile, and the real of peasant striking on the same ball, and she could have been so reserve of the other side of the regiment with him at the sense of her sister-in-law in the door of the station. All the heart of the strange was she was s\n",
    "\n",
    "\n",
    "---------- Epoch:300, Chars:46300, No of batches:32 Per batch:128, Layers:3 HiddenSize:256, Clip:5, Loss: 1.08 For polish\n",
    "a teraz na powietrzu, a po drugim siedzieć się nie pod niebiosa podniósł się w podwinał i począł głową podnieść na posłanie pod niebiosa pod niebiosa pod niebiosa pod niebiosa pod niebiosa pod niebiosa pod niebiosa pod niebiosa pod niebiosa pod niebiosa pod niebiosa pod niebiosa pod niebiosa pod niebiosa pod niebiosa pod niebiosa pod niebiosa pod niebiosa "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions:\n",
    "\n",
    "Increase in number of layers does not always help.\n",
    "\n",
    "Clip made the text more diverse, but also with some erros - words that does not exist. So you have to train a little longer.\n",
    "\n",
    "Increasing in the number of chars trained on greatly improved the sense of the text - pack of words started to have a meaning.\n",
    "\n",
    "Previously other function for one hot encoding was used, here called one_hot_encode_old. It gave different output than one_hot_encode. The difference was only at the last character of the output. So let say single example was 125 chars, so two outputs would differ only on last 125th position and it would be 0 in case of old function. Because of this single mistake in the input the network was not learning properly. It disturbed text generation so much that it stopped to be english.\n",
    "This shows vulnerability of LSTM to bad input.\n",
    "\n",
    "Examples:\n",
    "\n",
    "a great and advanced society has  aa  oa  nattee  aa  oa  nattee\n",
    "\n",
    "vs\n",
    "\n",
    "a great and advanced society has  to the state to the state to the\n",
    "\n",
    "Same approach works for polish language as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
