{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on: https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_chars = 32000\n",
    "\n",
    "with open('text', 'r') as fd:\n",
    "    full_text = fd.read().lower()\n",
    "full_text = full_text[0:no_of_chars]\n",
    "\n",
    "vocab = set(full_text)\n",
    "int2char = dict(enumerate(vocab))\n",
    "char2int = {char: ind for ind, char in int2char.items()}\n",
    "vocab_size = len(char2int)\n",
    "print(\"Vocabulary size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how LSTM layer works (sizes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_layers = 1\n",
    "batch_size = 2\n",
    "hidden_size = 3\n",
    "input_size = 3\n",
    "seq_size = 5\n",
    "\n",
    "lstm = nn.LSTM(input_size, hidden_size, no_of_layers, batch_first=True)\n",
    "\n",
    "inp = torch.randn(batch_size, seq_size, input_size)\n",
    "print(\"Input:\", inp)\n",
    "\n",
    "hidden_state = torch.randn(no_of_layers, batch_size, hidden_size)\n",
    "cell_state = torch.randn(no_of_layers, batch_size, hidden_size)\n",
    "print(\"Hidden:\", hidden_state)\n",
    "print(\"Cell:\", cell_state)\n",
    "out, full = lstm(inp, (hidden_state, cell_state))\n",
    "print(\"Full:\", full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM model.\n",
    "\n",
    "LSTM, differently from vanilla RNN, has two states: hidden state (short-term memory) and cell state (long-term memory).\n",
    "Together they have common name here: full_hidden.\n",
    "\n",
    "Below is the simple model consisiting from lstm layer and fully connected layer. Later we'll add dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers):\n",
    "        super(ModelLSTM, self).__init__()\n",
    "        output_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, n_layers, batch_first=True)   \n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x, full_hidden):\n",
    "        out, full_hidden = self.lstm(x, full_hidden)\n",
    "        out = out.contiguous().view(-1, self.hidden_size)\n",
    "        out = self.fc(out)\n",
    "        return out, full_hidden\n",
    "    \n",
    "    def init_full_hidden(self, batch_size):\n",
    "        hidden = torch.randn(self.n_layers, batch_size, self.hidden_size)\n",
    "        cell_state = torch.randn(self.n_layers, batch_size, self.hidden_size)\n",
    "        return (hidden, cell_state)\n",
    "    \n",
    "    def init_full_hidden_cuda(self, batch_size):\n",
    "        hidden = torch.randn(self.n_layers, batch_size, self.hidden_size).to(device)\n",
    "        cell_state = torch.randn(self.n_layers, batch_size, self.hidden_size).to(device)\n",
    "        return (hidden, cell_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_eq(text, no):\n",
    "    cnt = int(len(text) / no)\n",
    "    examples = [text[i:i+cnt] for i in range(0, len(text), cnt)]\n",
    "    if (no*cnt == len(text)):\n",
    "        return examples\n",
    "    else:\n",
    "        return examples[:-1]\n",
    "\n",
    "def produce_targets(examples):\n",
    "    targets = [ex[1:] for ex in examples]\n",
    "    inputs = [ex[:-1] for ex in examples]\n",
    "    return inputs, targets\n",
    "\n",
    "def translate_to_int(examples):\n",
    "    translated = [list(map(lambda ch: char2int[ch], ex)) for ex in examples]\n",
    "    return translated\n",
    "\n",
    "def translate_to_char(examples):\n",
    "    translated = [''.join(list(map(lambda i: int2char[i], ex))) for ex in examples]\n",
    "    return translated\n",
    "\n",
    "def one_hot_encode(examples):\n",
    "    features = np.zeros((len(examples), len(examples[0]), len(char2int)), dtype=np.float32)\n",
    "    \n",
    "    for i, example in enumerate(examples):\n",
    "        for pos in range(len(examples[i]) - 1):\n",
    "            features[i, pos, examples[i][pos]] = 1\n",
    "    return features\n",
    "\n",
    "def to_model_format(inputs):\n",
    "    if isinstance(inputs, str):\n",
    "        inputs = [inputs]\n",
    "    trans_inputs = translate_to_int(inputs)\n",
    "    encoded = one_hot_encode(trans_inputs)\n",
    "    encoded_tensor = torch.from_numpy(encoded)\n",
    "    return encoded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "no_of_examples = 32\n",
    "batch_size = examples_per_batch = 16\n",
    "n_epochs = 300\n",
    "lr = 0.0048\n",
    "\n",
    "no_of_batches = int(no_of_examples / examples_per_batch)\n",
    "\n",
    "examples = split_eq(full_text, no_of_examples)\n",
    "chars_per_example = len(examples[0])\n",
    "inputs, targets = produce_targets(examples)\n",
    "trans_inputs = translate_to_int(inputs)\n",
    "trans_targets = translate_to_int(targets)\n",
    "\n",
    "batches = []\n",
    "\n",
    "for i in range(no_of_batches):\n",
    "    input_seq = one_hot_encode(trans_inputs[i*examples_per_batch:(i+1)*examples_per_batch])\n",
    "    target_seq = torch.Tensor(trans_targets[i*examples_per_batch:(i+1)*examples_per_batch])\n",
    "    batches.append((torch.from_numpy(input_seq), target_seq))\n",
    "\n",
    "print(\"No of examples/No of data parts:\", no_of_examples)\n",
    "print(\"No of batches:\", no_of_batches)\n",
    "print(\"Examples per batch:\", examples_per_batch)\n",
    "print(\"Chars per example:\", chars_per_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how single batch item looks like.\n",
    "\n",
    "It has size (examples_per_batch, chars_per_example -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp, target = batches[0]\n",
    "print(target, target.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_size = len(char2int)\n",
    "\n",
    "model = ModelLSTM(input_size=dict_size, hidden_size=12, n_layers=3)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1300\n",
    "counter = 0\n",
    "print_every = 20\n",
    "\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    counter += 1\n",
    "    for batch in batches:\n",
    "        h = model.init_full_hidden(batch_size)\n",
    "        model.zero_grad()\n",
    "        inp, target = batch\n",
    "        output, h = model(inp, h)\n",
    "        loss = criterion(output, target.view(-1).long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if counter%print_every == 0:\n",
    "        print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
    "              \"Step: {}...\".format(counter),\n",
    "              \"Loss: {:.6f}...\".format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next(model, full_hidden, input_string):\n",
    "    encoded_input = to_model_format(input_string)\n",
    "    out, hidden = model(encoded_input, full_hidden)\n",
    "\n",
    "    # choosing one with highest probability\n",
    "    prob = nn.functional.softmax(out[-1], dim=0).data\n",
    "    char_ind = torch.max(prob, dim=0)[1].item()\n",
    "    return int2char[char_ind], hidden\n",
    "\n",
    "\n",
    "def run_model(model, starting_seq, size=50):\n",
    "    model.eval()\n",
    "    seq = starting_seq.lower()\n",
    "    h = model.init_full_hidden(1)\n",
    "    for _ in range(size):\n",
    "        char, h = predict_next(model, h, seq)\n",
    "        seq += char\n",
    "    return ''.join(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = run_model(model, 'character ')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some results for LSTM:\n",
    "\n",
    "ep: 1300, chars: 32000, batches: 2, examples: 32 Out: character tarraat ttrlsc  tprlmaas ttrlsc  tprlmaas ttrlsc  Loss:1.719\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "print(is_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_size = len(char2int)\n",
    "model = ModelLSTM(input_size=dict_size, hidden_size=36, n_layers=3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2000\n",
    "counter = 0\n",
    "print_every = 50\n",
    "\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    counter += 1\n",
    "    for batch in batches:\n",
    "        h = init_full_hidden_cuda(model, batch_size)\n",
    "        model.zero_grad()\n",
    "        inp, target = batch\n",
    "        inp, target = inp.to(device), target.to(device)\n",
    "        output, h = model(inp, h)\n",
    "        loss = criterion(output, target.view(-1).long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if counter%print_every == 0:\n",
    "        print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
    "              \"Step: {}...\".format(counter),\n",
    "              \"Loss: {:.6f}...\".format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next(device, model, full_hidden, input_string):\n",
    "    encoded_input = to_model_format(input_string)\n",
    "    out, hidden = model(encoded_input.to(device), full_hidden)\n",
    "\n",
    "    # choosing one with highest probability\n",
    "    prob = nn.functional.softmax(out[-1], dim=0).data\n",
    "    char_ind = torch.max(prob, dim=0)[1].item()\n",
    "    return int2char[char_ind], hidden\n",
    "\n",
    "\n",
    "def run_model(device, model, starting_seq, size=50):\n",
    "    model.eval()\n",
    "    seq = starting_seq.lower()\n",
    "    h = model.init_full_hidden_cuda(1)\n",
    "    for _ in range(size):\n",
    "        char, h = predict_next(device, model, h, seq)\n",
    "        seq += char\n",
    "    return ''.join(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = run_model(device, model, 'character ', 50)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some results:\n",
    "\n",
    "ep: 1500, chars: 32000, batches: 2, examples: 32 hidden:24 \n",
    "Out: character asde ttrruus it wutt sglttn lfa  tgu  tge  ggnlps Loss:1.198\n",
    "\n",
    "ep: 2000, chars: 32000, batches: 2, examples: 32 hidden:36 \n",
    "Out: character sandlodgrrktions it sam yinssaotdirspaiettwdnnstt  Loss:0.744\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
